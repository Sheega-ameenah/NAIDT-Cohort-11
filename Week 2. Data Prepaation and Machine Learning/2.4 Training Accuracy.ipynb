{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Evaluation Metrics\n",
    "\n",
    "Machine learning evaluation metrics are used to assess the performance and quality of machine learning models. These metrics help us understand how well our models are performing and provide insights into their strengths and weaknesses. Let's explore some commonly used evaluation metrics along with their formulas and examples:\n",
    "\n",
    "1. **Accuracy**: Accuracy measures the proportion of correct predictions compared to the total number of predictions. It is suitable for balanced datasets where the classes are equally represented. However, it can be misleading when dealing with imbalanced datasets.\n",
    "- Formula: $$\\frac{TP + TN} {TP + TN + FP + FN}$$\n",
    "\n",
    "        TP: True Positives, TN: True Negatives, FP: False Positives, FN: False Negatives\n",
    "\n",
    "- Example:\n",
    "\n",
    "| Actual   | Predicted |\n",
    "|----------|-----------|\n",
    "| Yes      | Yes       |\n",
    "| No       | No        |\n",
    "| Yes      | No        |\n",
    "| Yes      | Yes       |\n",
    "| Yes      | Yes       |\n",
    "\n",
    " $Accuracy =\\frac{3 + 1} {5} = 0.8$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Precision and Recall**: Precision and recall are metrics used in binary classification problems, particularly when the classes are imbalanced. Precision measures the proportion of correctly predicted positive instances out of all predicted positives. Recall, also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances out of all actual positives.\n",
    "\n",
    "**Precision Formula**: $$\\frac{TP}{TP + FP}$$\n",
    "**Recall Formula:** $$\\frac{TP}{TP + FN}$$\n",
    "**Example:**\n",
    "\n",
    "\n",
    "|Actual | Predicted|\n",
    "|-------|-------|\n",
    "|Yes|Yes|\n",
    "|No|\tYes|\n",
    "|Yes|\tNo|\n",
    "|Yes|\tYes|\n",
    "|No|\tNo|\n",
    "\n",
    "$$Precision = \\frac{2}{2 + 1} = 0.67$$\n",
    "$$Recall = \\frac{2}{2 + 1} = 0.67$$\n",
    "\n",
    "\n",
    "3. **F1 Score**: The F1 score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is useful when we want to consider both metrics together.\n",
    "\n",
    "**Formula**: $$ \\frac{2 \\times Precision \\times Recall}{Precision + Recall} $$\n",
    "Example:\n",
    "$$F1 Score =  \\frac {2 \\times 0.67 \\times 0.67 }{0.67 + 0.67} = 0.67$$\n",
    "\n",
    "4. **Mean Squared Error (MSE)**: MSE is commonly used for regression problems. It measures the average squared difference between the predicted and true values. A lower MSE indicates better performance.\n",
    "\n",
    "**Formula** : $$ \\frac{1}{n} \\times Σ(y_true - y_pred)^2$$\n",
    "**Example** :\n",
    "|y_true|\ty_pred|\n",
    "|-------|-------|\n",
    "|2|\t1.5|\n",
    "|3|\t2.0|\n",
    "|5|\t4.5|\n",
    "|4|\t3.5|\n",
    "|6|\t5.5|\n",
    "\n",
    "$$MSE = \\frac{1}{5} \\times ((2 - 1.5)^2 + (3 - 2.0)^2 + (5 - 4.5)^2 + (4 - 3.5)^2 + (6 - 5.5)^2) = 0.25$$\n",
    "5. **R-squared (Coefficient of Determination)**: R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, with 1 indicating a perfect fit and 0 indicating no linear relationship.\n",
    "\n",
    "**Formula**: $ 1 - \\frac{SSR}{SST}$\n",
    "\n",
    "*SSR: Sum of Squared Residuals, SST: Total Sum of Squares*\n",
    "\n",
    "**Example**:\n",
    "$$R-squared = 1 - \\frac{SSR}{SST} = 1 - \\frac{0.25}{10} = 0.975$$\n",
    "\n",
    "6. **ROC-AUC**: Receiver Operating Characteristic (ROC) curve and Area Under the Curve (AUC) are used for binary classification problems. ROC-AUC evaluates the model's ability to distinguish between classes by plotting the true positive rate against the false positive rate.\n",
    "\n",
    "*ROC Curve*: Plots True Positive Rate (TPR) against False Positive Rate (FPR)\n",
    "\n",
    "*AUC*: Area Under the ROC Curve\n",
    "\n",
    "7. **Log Loss (Logistic Loss)**: Log loss is commonly used for probabilistic classification problems. It measures the logarithm of the likelihood of the predicted probabilities matching the true labels. A lower log loss indicates better calibration of probabilities.\n",
    "\n",
    "**Formula**: $$-\\frac{1}{n} \\times Σ(y_true \\times log(y_pred) + (1 - y_true) \\times log(1 - y_pred))$$\n",
    "\n",
    "8. **Confusion Matrix**: A confusion matrix provides a comprehensive summary of the model's performance, especially in multi-class classification problems. It displays the counts of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "Provides counts of *True Positives (TP)*, *True Negatives (TN)*, *False Positives (FP)*, and *False Negatives (FN)*\n",
    "\n",
    "**Example**: See the table with counts of each category.\n",
    "\n",
    "It' s important to choose the appropriate evaluation metric based on the specific problem and requirements. Different metrics provide different insights into the model's performance, and it's often recommended to consider multiple metrics together to gain a comprehensive understanding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split in Machine Learning\n",
    "\n",
    "Train-test split is a common technique used in machine learning to evaluate the performance of a model on unseen data. It involves splitting the available dataset into two subsets: one for training the model (the training set) and the other for evaluating the model's performance (the test set). The train-test split helps assess how well the model generalizes to new, unseen data.\n",
    "\n",
    "we'll be using another library from SKLearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset:\n",
    "df = pd.read_csv(\"Cleaned House Data.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>crime_rate</th>\n",
       "      <th>resid_area</th>\n",
       "      <th>air_qual</th>\n",
       "      <th>room_num</th>\n",
       "      <th>age</th>\n",
       "      <th>teachers</th>\n",
       "      <th>poor_prop</th>\n",
       "      <th>airport</th>\n",
       "      <th>n_hos_beds</th>\n",
       "      <th>n_hot_rooms</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>waterbody_Encoded</th>\n",
       "      <th>avg_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>32.31</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>24.7</td>\n",
       "      <td>4.98</td>\n",
       "      <td>1</td>\n",
       "      <td>5.480</td>\n",
       "      <td>11.19200</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "      <td>0.026944</td>\n",
       "      <td>37.07</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>22.2</td>\n",
       "      <td>9.14</td>\n",
       "      <td>0</td>\n",
       "      <td>7.332</td>\n",
       "      <td>12.17280</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.9675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "      <td>0.026924</td>\n",
       "      <td>37.07</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>22.2</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0</td>\n",
       "      <td>7.394</td>\n",
       "      <td>46.19856</td>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "      <td>0.031857</td>\n",
       "      <td>32.18</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>21.3</td>\n",
       "      <td>2.94</td>\n",
       "      <td>1</td>\n",
       "      <td>9.268</td>\n",
       "      <td>11.26720</td>\n",
       "      <td>45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "      <td>0.066770</td>\n",
       "      <td>32.18</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>21.3</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0</td>\n",
       "      <td>8.824</td>\n",
       "      <td>11.28960</td>\n",
       "      <td>55</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  crime_rate  resid_area  air_qual  room_num   age  teachers  \\\n",
       "0   24.0    0.006300       32.31     0.538     6.575  65.2      24.7   \n",
       "1   21.6    0.026944       37.07     0.469     6.421  78.9      22.2   \n",
       "2   34.7    0.026924       37.07     0.469     7.185  61.1      22.2   \n",
       "3   33.4    0.031857       32.18     0.458     6.998  45.8      21.3   \n",
       "4   36.2    0.066770       32.18     0.458     7.147  54.2      21.3   \n",
       "\n",
       "   poor_prop  airport  n_hos_beds  n_hot_rooms  rainfall  waterbody_Encoded  \\\n",
       "0       4.98        1       5.480     11.19200        23                1.0   \n",
       "1       9.14        0       7.332     12.17280        42                2.0   \n",
       "2       4.03        0       7.394     46.19856        38                0.0   \n",
       "3       2.94        1       9.268     11.26720        45                2.0   \n",
       "4       5.33        0       8.824     11.28960        55                2.0   \n",
       "\n",
       "   avg_dist  \n",
       "0    4.0875  \n",
       "1    4.9675  \n",
       "2    4.9675  \n",
       "3    6.0650  \n",
       "4    6.0625  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"price\", axis=1)\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are splitting the data into a training set (80% of the data) and a test set (20% of the data). The random_state parameter ensures reproducibility of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split is crucial to assess the model's performance on unseen data and avoid overfitting. The model is trained on the training set and then evaluated on the test set to measure its ability to generalize. It helps identify potential issues such as underfitting or overfitting.\n",
    "\n",
    "It's important to note that the train-test split should be representative of the original data to ensure reliable evaluation. The choice of the test set size depends on the dataset size, available data, and specific requirements of the problem at hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.729439440716198"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_train, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting in Machine Learning\n",
    "\n",
    "In machine learning, overfitting and underfitting are two common problems that occur when training models. Understanding these concepts is crucial for building robust and accurate machine learning models. Let's take a closer look at each:\n",
    "\n",
    "### 1. Overfitting:\n",
    "\n",
    "Overfitting occurs when a model performs exceptionally well on the training data but fails to generalize well on new, unseen data. It happens when the model becomes too complex or overly flexible, effectively \"memorizing\" the training data instead of learning the underlying patterns. Key characteristics of overfitting include:\n",
    "\n",
    "- The model shows low training error but high test error.\n",
    "- The model captures noise and irrelevant details in the training data.\n",
    "- The model may have excessive complexity, such as having too many features or high polynomial degrees.\n",
    "\n",
    "Remedies for overfitting:\n",
    "\n",
    "- Increase the size of the training dataset.\n",
    "- Reduce the complexity of the model, such as by decreasing the number of features or applying feature selection techniques.\n",
    "- Regularize the model using techniques like L1 or L2 regularization.\n",
    "- Use cross-validation for hyperparameter tuning.\n",
    "### 2. Underfitting:\n",
    "Underfitting occurs when a model is too simple or inflexible to capture the underlying patterns in the training data. It fails to learn the relationships between the features and the target variable, resulting in poor performance on both the training and test data. Key characteristics of underfitting include:\n",
    "\n",
    "- The model shows high training error and high test error.\n",
    "- The model is too simplistic to capture the complexity of the data.\n",
    "- The model may have insufficient features or inadequate training time.\n",
    "\n",
    "Remedies for underfitting:\n",
    "\n",
    "- Increase the complexity of the model, such as by adding more features or increasing the model's capacity.\n",
    "- Perform feature engineering to extract more meaningful features from the data.\n",
    "- Increase the training time or adjust the learning rate for iterative algorithms.\n",
    "\n",
    "Balancing between overfitting and underfitting is a crucial aspect of model training. The goal is to find the right level of complexity that allows the model to generalize well to new, unseen data. Techniques like cross-validation, regularization, and hyperparameter tuning play important roles in mitigating overfitting and underfitting.\n",
    "\n",
    "It's essential to monitor the model's performance on both the training and test data and make adjustments accordingly. Regular evaluation and fine-tuning of the model help strike the right balance between complexity and generalization, leading to more accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.729439440716198"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6571081693323386"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_test_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Feature selection is an important step in machine learning and data analysis that involves selecting a subset of relevant features from the original set of available features. It aims to improve model performance, reduce computational complexity, and enhance interpretability by focusing on the most informative and influential features. Feature selection helps mitigate the curse of dimensionality and can lead to more accurate and efficient models. \n",
    "Subset and shrinkage methods are two common approaches to feature selection. Let's take a closer look at each:\n",
    "\n",
    "### 1.  Subset Selection Methods:\n",
    "Subset selection methods aim to find the best subset of features that maximizes the model's performance. There are two main types of subset selection methods:\n",
    "\n",
    "- Forward Selection: This method starts with an empty set of features and iteratively adds the most significant feature that improves the model's performance the most until a stopping criterion is met.\n",
    "- Backward Elimination: This method starts with the full set of features and iteratively removes the least significant feature that has the least impact on the model's performance until a stopping criterion is met.\n",
    "\n",
    "Subset selection methods evaluate the performance of different feature subsets using metrics like cross-validation error or information criteria. They can be computationally expensive, especially for large feature sets, but they provide an optimal subset of features based on the evaluation criterion.\n",
    "\n",
    "### 2. Shrinkage Methods:\n",
    "Shrinkage methods, also known as regularization methods, add a penalty term to the objective function during model training. These methods encourage sparse solutions, effectively shrinking less important feature coefficients towards zero. Two popular shrinkage methods are:\n",
    "\n",
    "- Lasso Regression: Lasso regression applies L1 regularization, resulting in sparse solutions where some feature coefficients become exactly zero. It automatically performs feature selection by effectively excluding irrelevant features from the model.\n",
    "- Ridge Regression: Ridge regression applies L2 regularization, which reduces the magnitude of feature coefficients without excluding any feature entirely. While ridge regression does not perform explicit feature selection, it can still reduce the impact of less important features.\n",
    "\n",
    "Shrinkage methods provide a trade-off between model complexity and overfitting. They can effectively handle high-dimensional datasets and multicollinearity by reducing the impact of less relevant features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime_rate:\t\t\t -0.4188348647708363\n",
      "resid_area:\t\t\t -0.009670598602540215\n",
      "air_qual:\t\t\t -14.799264505460645\n",
      "room_num:\t\t\t 4.4945488635581645\n",
      "age:\t\t\t -0.008730671784292014\n",
      "teachers:\t\t\t 0.9318093769750563\n",
      "poor_prop:\t\t\t -0.5684201491712669\n",
      "airport:\t\t\t 1.1024770184686619\n",
      "n_hos_beds:\t\t\t 0.2943603909476938\n",
      "n_hot_rooms:\t\t\t 0.09245739022506491\n",
      "rainfall:\t\t\t 0.02620869785341541\n",
      "waterbody_Encoded:\t\t\t -0.03835395999776788\n",
      "avg_dist:\t\t\t -1.2631951142871494\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names\n",
    "feature_names = X_train.columns  # Replace <your_feature_names> with the actual feature names from your dataset\n",
    "\n",
    "# Get the coefficients\n",
    "coefficients = lr.coef_\n",
    "\n",
    "# Print feature names and coefficients\n",
    "for feature, coef in zip(feature_names, coefficients):\n",
    "    print(f\"{feature}:\\t\\t\\t {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [feature for feature, coef in zip(feature_names, coefficients) if coef > 0.5 or coef < -0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['air_qual', 'room_num', 'teachers', 'poor_prop', 'airport', 'avg_dist']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected = X_train[selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_selected = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model_selected.fit(X_selected, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_selected_pred = reg_model_selected.predict(X_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.729439440716198"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7246273222575257"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, y_selected_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_selected_pred = reg_model_selected.predict(X_test[selected_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6571081693323386"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.662103466590333"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_test_selected_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset selection and shrinkage methods offer different approaches to feature selection, each with its own advantages and considerations. The choice between them depends on the specific problem, the size of the feature set, and the desired balance between model complexity and interpretability. Experimentation and evaluation of different feature selection methods are important to identify the most relevant subset of features for a given problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Regression Models\n",
    "\n",
    "Linear regression is a widely used regression model, but there are several other regression models that can be applied to different types of data and scenarios. Here are a few examples:\n",
    "\n",
    "#### 1. Ridge Regression:\n",
    "Ridge regression is a regularization technique that adds an L2 penalty term to the linear regression objective function. It helps reduce overfitting by shrinking the coefficients towards zero. Ridge regression is particularly useful when dealing with multicollinearity in the data.\n",
    "\n",
    "#### 2. Lasso Regression:\n",
    "Lasso regression is another regularization technique that adds an L1 penalty term to the linear regression objective function. It encourages sparsity by setting some coefficients to exactly zero, effectively performing feature selection. Lasso regression is helpful when you want to identify the most relevant features for prediction.\n",
    "\n",
    "#### 3. ElasticNet Regression:\n",
    "ElasticNet regression combines both L1 and L2 regularization terms in the linear regression objective function. It offers a balance between the feature selection capability of Lasso regression and the coefficient shrinkage of Ridge regression. ElasticNet regression is useful when dealing with high-dimensional datasets and multicollinearity.\n",
    "\n",
    "#### 4. Decision Tree Regression:\n",
    "Decision tree regression builds a regression model by recursively partitioning the data based on feature values. It predicts the target variable based on the average target value of the training instances within each leaf node. Decision tree regression can capture complex relationships and handle both numerical and categorical features.\n",
    "\n",
    "#### 5. Random Forest Regression:\n",
    "Random forest regression is an ensemble method that combines multiple decision trees to make predictions. It improves the predictive accuracy and handles overfitting by averaging the predictions of multiple trees. Random forest regression is robust, can handle high-dimensional data, and provides feature importance rankings.\n",
    "\n",
    "\n",
    "These are just a few examples of regression models beyond linear regression. Each model has its own strengths and assumptions, and the choice of model depends on the specific problem, the nature of the data, and the desired trade-offs between accuracy, interpretability, and computational complexity. It's important to understand the characteristics of different regression models and experiment with them to find the most suitable one for your specific task.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Ridge Regression:</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Train the ridge regression model\n",
    "ridge_reg = Ridge(alpha=0.5)  # Specify the regularization strength (alpha)\n",
    "ridge_reg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ridge = ridge_reg.predict(X_train)\n",
    "y_test_ridge = ridge_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7287998343826629"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, y_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6545725618645957"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_test_ridge)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Train the lasso regression model\n",
    "lasso_reg = Lasso(alpha=0.1)  # Specify the regularization strength (alpha)\n",
    "lasso_reg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lasso = lasso_reg.predict(X_train)\n",
    "y_test_lasso = lasso_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7204680771169034"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, y_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6493647009418789"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_test_lasso)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticNet Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=0.5)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Assuming X and y are your feature matrix and target variable, respectively\n",
    "\n",
    "# Train the elastic net regression model\n",
    "elastic_net = ElasticNet(alpha=0.5, l1_ratio=0.5)  # Specify the regularization strengths (alpha, l1_ratio)\n",
    "elastic_net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_elast = elastic_net.predict(X_train)\n",
    "y_test_elast = elastic_net.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6932751810792401"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, y_elast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6648641852012285"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_test_elast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Assuming X and y are your feature matrix and target variable, respectively\n",
    "\n",
    "# Train the decision tree regression model\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
